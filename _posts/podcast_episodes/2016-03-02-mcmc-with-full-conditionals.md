---
layout: episode
category: podcast
title: Markov Chain Montecarlo with full conditionals 
date: 2016-03-02
keywords: [mcmc, big data, statistics, data science, hamiltonian, machine learning, sampling]
producer: worldofpiggy.com
explicit: No
block: not available
duration: "17:58"
length: 16400000
media: https://s3.amazonaws.com/dshpodcast/media/datascience_episode_mcmc.mp3
excerpt: At some point, statistical problems need sampling. Sampling consists in generating observations from a specific distribution.
comments: true
---


<img src="https://s3.amazonaws.com/dshpodcast/media/cover.jpg" />



Markov Chain MonteCarlo with full conditional calculations

At some point, statistical problems need sampling. Sampling consists in generating observations from a specific distribution.
Prior knowledge and likelihood are the essential components of bayesian theory to model a number of real-world problems. Therefore, bayesian statistics is based on the fundamental task of sampling from a distribution, as complicated as it can be, and then compute summary statistics like mean and variance in order to describe the observations that are representing the assumptions (the model).

In this episode, we learn how to do this.

In addition, I explain how Hamiltonian Montecarlo sampling works and why we should all use it, whenever we can.
Following the [shownotes](https://s3-eu-west-1.amazonaws.com/wopcontent/uploads/2016/03/Mcmc-full-conditional.pdf) makes it easier to understand some mathematical formulas that is better to read than listen.

Enjoy the show!

